{"cells":[{"cell_type":"markdown","metadata":{"id":"qC71Qv1-TtI-"},"source":["\n","Solution template for the question 1.6-1.7. This template consists of following steps. Except the step 2, you don't need to modify it to answer the questions.\n","1.   Initialize libraries\n","2.   **Insert the answers for the questions 1.1~1.5 in q1_solution (this is the part you need to fill)**\n","3.   Define data loaders\n","4.   Define VAE network architecture\n","5.   Initialize the model and optimizer\n","6.   Train the model\n","7.   Save the model\n","8.   Load the model\n","9.   Evaluate the model with importance sampling"]},{"cell_type":"markdown","metadata":{"id":"mcs7QFvETxQJ"},"source":["Initialize libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLoP5GRpEPbI"},"outputs":[],"source":["import math\n","from torchvision.datasets import utils\n","import torch.utils.data as data_utils\n","import torch\n","import os\n","import numpy as np\n","from torch import nn\n","from torch.nn.modules import upsampling\n","from torch.functional import F\n","from torch.optim import Adam"]},{"cell_type":"markdown","metadata":{"id":"NTB40neeR6-k"},"source":["Complete **functions in q1_solution** to answer the questions 1.1~1.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Kr08AArNlHU"},"outputs":[],"source":["from q1_solution import log_likelihood_bernoulli, log_likelihood_normal, log_mean_exp, kl_gaussian_gaussian_analytic, kl_gaussian_gaussian_mc"]},{"cell_type":"markdown","metadata":{"id":"r3v_ld3ITRFl"},"source":["Define data loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oiK4L0TdETNb"},"outputs":[],"source":["def get_data_loader(dataset_location, batch_size):\n","    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n","    # start processing\n","    def lines_to_np_array(lines):\n","        return np.array([[int(i) for i in line.split()] for line in lines])\n","    splitdata = []\n","    for splitname in [\"train\", \"valid\", \"test\"]:\n","        filename = \"binarized_mnist_%s.amat\" % splitname\n","        filepath = os.path.join(dataset_location, filename)\n","        utils.download_url(URL + filename, dataset_location)\n","        with open(filepath) as f:\n","            lines = f.readlines()\n","        x = lines_to_np_array(lines).astype('float32')\n","        x = x.reshape(x.shape[0], 1, 28, 28)\n","        # pytorch data loader\n","        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n","        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n","        splitdata.append(dataset_loader)\n","    return splitdata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZsL1gLLEVJM","executionInfo":{"status":"ok","timestamp":1650232680292,"user_tz":240,"elapsed":14812,"user":{"displayName":"Yass M","userId":"05580048517581510519"}},"outputId":"e24fa8fe-13e3-4fcc-e090-16db0d0bd903"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: binarized_mnist/binarized_mnist_train.amat\n","Using downloaded and verified file: binarized_mnist/binarized_mnist_valid.amat\n","Using downloaded and verified file: binarized_mnist/binarized_mnist_test.amat\n"]}],"source":["train, valid, test = get_data_loader(\"binarized_mnist\", 64)"]},{"cell_type":"markdown","metadata":{"id":"8PoFxey7TUFS"},"source":["Define VAE network architecture\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POBmU6UCEb4l"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","class Encoder(nn.Module):\n","    def __init__(self, latent_size):\n","        super(Encoder, self).__init__()\n","        self.mlp = nn.Sequential(\n","            nn.Linear(784, 300),\n","            nn.ELU(),\n","            nn.Linear(300, 300),\n","            nn.ELU(),\n","            nn.Linear(300, 2 * latent_size),\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x=x.to(device)\n","        z_mean, z_logvar = self.mlp(x.view(batch_size, 784)).chunk(2, dim=-1)\n","        return z_mean, z_logvar\n","\n","class Decoder(nn.Module):\n","    def __init__(self, latent_size):\n","        super(Decoder, self).__init__()\n","        self.mlp = nn.Sequential(\n","            nn.Linear(latent_size, 300),\n","            nn.ELU(),\n","            nn.Linear(300, 300),\n","            nn.ELU(),\n","            nn.Linear(300, 784),\n","        )\n","        \n","    def forward(self, z):\n","        z=z.to(device)\n","        return self.mlp(z) - 5.\n","\n","class VAE(nn.Module):\n","    def __init__(self, latent_size):\n","        super(VAE, self).__init__()\n","        self.encode = Encoder(latent_size)\n","        self.decode = Decoder(latent_size)\n","\n","    def forward(self, x):\n","        x=x.to(device)\n","        z_mean, z_logvar = self.encode(x)\n","        z_sample = z_mean + torch.exp(z_logvar / 2.) * torch.randn_like(z_logvar)\n","        x_mean = self.decode(z_sample)\n","        return z_mean, z_logvar, x_mean\n","\n","    def loss(self, x, z_mean, z_logvar, x_mean):\n","        ZERO = torch.zeros(z_mean.size()).to(device)\n","        x, z_mean, z_logvar, x_mean = x.to(device), z_mean.to(device), z_logvar.to(device), x_mean.to(device)\n","        kl = kl_gaussian_gaussian_mc(z_mean, z_logvar, ZERO, ZERO, num_samples=1000).mean()\n","        #kl = kl_gaussian_gaussian_analytic(z_mean, z_logvar, ZERO, ZERO).mean()\n","        recon_loss = -log_likelihood_bernoulli(\n","            torch.sigmoid(x_mean.view(x.size(0), -1)),\n","            x.view(x.size(0), -1),            \n","        ).mean()\n","        return recon_loss + kl"]},{"cell_type":"markdown","metadata":{"id":"Phg07ERvTYuh"},"source":["Initialize a model and optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xTxgDwZfEesO","outputId":"c1584bb4-ad9d-46f0-daab-4703d162a8f4","executionInfo":{"status":"ok","timestamp":1650232680294,"user_tz":240,"elapsed":22,"user":{"displayName":"Yass M","userId":"05580048517581510519"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["VAE(\n","  (encode): Encoder(\n","    (mlp): Sequential(\n","      (0): Linear(in_features=784, out_features=300, bias=True)\n","      (1): ELU(alpha=1.0)\n","      (2): Linear(in_features=300, out_features=300, bias=True)\n","      (3): ELU(alpha=1.0)\n","      (4): Linear(in_features=300, out_features=200, bias=True)\n","    )\n","  )\n","  (decode): Decoder(\n","    (mlp): Sequential(\n","      (0): Linear(in_features=100, out_features=300, bias=True)\n","      (1): ELU(alpha=1.0)\n","      (2): Linear(in_features=300, out_features=300, bias=True)\n","      (3): ELU(alpha=1.0)\n","      (4): Linear(in_features=300, out_features=784, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["vae = VAE(100)\n","vae=vae.to(device)\n","params = vae.parameters()\n","optimizer = Adam(params, lr=3e-4)\n","print(vae)"]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"hVgLqhK6e903"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oqw9SI7aTdtG"},"source":["Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"SWtQakAOEhxN","executionInfo":{"status":"error","timestamp":1650232681418,"user_tz":240,"elapsed":1142,"user":{"displayName":"Yass M","userId":"05580048517581510519"}},"outputId":"a33fc836-f7c1-46f4-d8ef-3a21fb4b6b69"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b8cbe3f9b672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_logvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_logvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.38 GiB (GPU 0; 14.76 GiB total capacity; 9.67 GiB already allocated; 1.46 GiB free; 11.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["for i in range(20):\n","    # train\n","    for x in train:\n","        x=x.to(device)\n","        optimizer.zero_grad()\n","        z_mean, z_logvar, x_mean = vae(x.to(device))\n","        loss = vae.loss(x, z_mean, z_logvar, x_mean)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # evaluate ELBO on the valid dataset\n","    with torch.no_grad():\n","        total_loss = 0.\n","        total_count = 0\n","        for x in valid:\n","            total_loss += vae.loss(x, *vae(x)) * x.size(0)\n","            total_count += x.size(0)\n","        print('-elbo: ', (total_loss / total_count).item())"]},{"cell_type":"markdown","metadata":{"id":"HXp5vuhDTg1J"},"source":["Save the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYfmW5TAElEO"},"outputs":[],"source":["torch.save(vae, 'model.pt')"]},{"cell_type":"markdown","metadata":{"id":"8Iz6QX_KTizK"},"source":["Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hqcb8BrmEnMh"},"outputs":[],"source":["vae = torch.load('model.pt')"]},{"cell_type":"markdown","metadata":{"id":"OTpoVRncTmSR"},"source":["Evaluate the $\\log p_\\theta(x)$ of the model on test by using importance sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tc2q6dxgEsIh"},"outputs":[],"source":["total_loss = 0.\n","total_count = 0\n","with torch.no_grad():\n","    #x = next(iter(test))\n","    for x in test:\n","        # init\n","        K = 200\n","        M = x.size(0)\n","        x = x.to(device)\n","        # Sample from the posterior\n","        z_mean, z_logvar = vae.encode(x)\n","        #z_mean,z_logvar = z_mean.to(device),z_logvar.to(device)\n","        eps = torch.randn(z_mean.size(0), K, z_mean.size(1))\n","        eps = eps.to(device)\n","        z_samples = z_mean[:, None, :] + torch.exp(z_logvar / 2.)[:, None, :] * eps # Broadcast the noise over the mean and variance\n","\n","        # Decode samples\n","        z_samples_flat = z_samples.view(-1, z_samples.size(-1)) # Flatten out the z samples\n","        x_mean_flat = vae.decode(z_samples_flat) # Push it through\n","\n","        # Reshape images and posterior to evaluate probabilities\n","        x_flat = x[:, None].repeat(1, K, 1, 1, 1).reshape(M*K, -1)\n","        z_mean_flat = z_mean[:, None, :].expand_as(z_samples).reshape(M*K, -1)\n","        z_logvar_flat =  z_logvar[:, None, :].expand_as(z_samples).reshape(M*K, -1)\n","        ZEROS = torch.zeros(z_mean_flat.size()).to(device)\n","\n","        # Calculate all the probabilities!\n","        log_p_x_z = log_likelihood_bernoulli(torch.sigmoid(x_mean_flat), x_flat).view(M, K)\n","        log_q_z_x = log_likelihood_normal(z_mean_flat, z_logvar_flat, z_samples_flat).view(M, K)\n","        log_p_z = log_likelihood_normal(ZEROS, ZEROS, z_samples_flat).view(M, K)\n","\n","        # Recombine them.\n","        w = log_p_x_z + log_p_z - log_q_z_x\n","        log_p = log_mean_exp(w)\n","\n","        # Accumulate\n","        total_loss += log_p.sum()\n","        total_count += M\n","      \n","print('log p(x):', (total_loss / total_count).item())"]},{"cell_type":"code","source":[],"metadata":{"id":"FXH3WIGz3foI"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}